{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Graph Library (DGL)\n",
    "=====================\n",
    "\n",
    "DGL is designed to bring machine learning closer to graph-structured data. Specifically DGL enables trouble-free implementation of graph neural network (GNN) model family. Unlike PyTorch or Tensorflow, DGL provides friendly APIs to perform the fundamental operations in GNNs such as message passing and reduction. Through DGL, we hope to benefit both researchers trying out new ideas and engineers in production.\n",
    "\n",
    "In this tutorial, we demostrate the basics of DGL including:\n",
    "- How to create a graph?\n",
    "- How to manipulate node/edge features on a graph?\n",
    "- How to convert a graph to/from other formats?\n",
    "- How to perform message passing computation on a graph?\n",
    "- How to implement a Graph Convolutional Network?\n",
    "- How to batch execution of multiple graphs?\n",
    "- How to perform efficient read-out on a batch of graphs?\n",
    "\n",
    "Although this tutorial uses [PyTorch](https://pytorch.org) as backend for tensor-related computations (thus some familarity with PyTorch is preferred), DGL is designed to be platform-agnostic and can be seamlessly integreted into other frameworks like [MXNet](https://mxnet.apache.org/) and [TensorFlow](https://www.tensorflow.org/), and we are actively working on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup, just ignore this cell\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['animation.html'] = 'html5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the well-knowned *\"Zachary's karate club\"* social network. The network captures 34 members of a karate club, documenting pairwise links between members who interacted outside the club. The club later splits into two communities led by the instructor (node 0) and club president (node 33). You could read more about the story in the [wiki page](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) A visualization of the network and the community is as follows:\n",
    "\n",
    "![karate](https://www.dropbox.com/s/d7dgs4fantje3dg/karate.jpg?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: creating a graph\n",
    "-----------------------------------\n",
    "\n",
    "Let's see how we can create such a graph in DGL. We start with importing `dgl` and other relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create an empty `DGLGraph`. In DGL, nodes are consecutive integers start from 0. The following codes add all the club members into this graph (34 nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = dgl.DGLGraph()\n",
    "G.add_nodes(34)\n",
    "print('Number of nodes:', G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Karate Club network contains 78 edges:\n",
    "```\n",
    "[1 0]\n",
    "[2 0] [2 1]\n",
    "[3 0] [3 1] [3 2]\n",
    "[4 0]\n",
    "[5 0]\n",
    "[6 0] [6 4] [6 5]\n",
    "[7 0] [7 1] [7 2] [7 3]\n",
    "[8 0] [8 2]\n",
    "[9 2]\n",
    "[10 0] [10 4] [10 5]\n",
    "[11 0]\n",
    "[12 0] [12 3]\n",
    "[13 0] [13 1] [13 2] [13 3]\n",
    "[16 5] [16 6]\n",
    "[17 0] [17 1]\n",
    "[19 0] [19 1]\n",
    "[21 0] [21 1]\n",
    "[25 23] [25 24]\n",
    "[27 2] [27 23] [27 24]\n",
    "[28 2]\n",
    "[29 23] [29 26]\n",
    "[30 1] [30 8]\n",
    "[31 0] [31 24] [31 25] [31 28]\n",
    "[32 2] [32 8] [32 14] [32 15] [32 18] [32 20] [32 22] [32 23] [32 29] [32 30] [32 31]\n",
    "[33 8] [33 9] [33 13] [33 14] [33 15] [33 18] [33 19] [33 20] [33 22] [33 23] [33 26] [33 27] [33 28] [33 29] [33 30] [33 31] [33 32]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DGL, edges can be added by specifying the two endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edge(1, 0)\n",
    "print('Now we have %d edges!' % G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add multiple edges at once, use a list/tensor of nodes to specify the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "########\n",
    "# NOTE: in DGL, edges are added by specifying a list of source nodes and a list of destination nodes,\n",
    "# rather than a list of source-destination node pairs. This is different from other popular graph\n",
    "# package such as networkx, python-igraph.\n",
    "\n",
    "########\n",
    "# NOTE: edges in DGLGraphs are all directional.\n",
    "\n",
    "# add two edges 2->0 and 2->1 using list\n",
    "G.add_edges([2, 2], [0, 1])\n",
    "\n",
    "# add three edges 3->0, 3->1 and 3->2 using torch tensor\n",
    "src = torch.tensor([3, 3, 3])\n",
    "dst = torch.tensor([0, 1, 2])\n",
    "G.add_edges(src, dst)\n",
    "\n",
    "print('Now we have %d edges!' % G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two edges 4->0, 5->0 using list\n",
    "G.add_edges([4, 5], 0)\n",
    "\n",
    "# add three edges 6->0 6->4 6->5 using torch tensor\n",
    "G.add_edges(6, torch.tensor([0, 4, 5]))\n",
    "\n",
    "print('Now we have %d edges!' % G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the edges share the same source or destination nodes, the list/tensor type can be replaced with a single integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise: please finish the karate club graph by adding the remaining edges. We have provided you all the\n",
    "# remaining edge tuples in a list.\n",
    "\n",
    "edge_list = [(7, 0), (7, 1), (7, 2), (7, 3), (8, 0), (8, 2), (9, 2), (10, 0), (10, 4), (10, 5),\n",
    "             (11, 0), (12, 0), (12, 3), (13, 0), (13, 1), (13, 2), (13, 3), (16, 5), (16, 6),\n",
    "             (17, 0), (17, 1), (19, 0), (19, 1), (21, 0), (21, 1), (25, 23), (25, 24), (27, 2),\n",
    "             (27, 23), (27, 24), (28, 2), (29, 23), (29, 26), (30, 1), (30, 8), (31, 0), (31, 24),\n",
    "             (31, 25), (31, 28), (32, 2), (32, 8), (32, 14), (32, 15), (32, 18), (32, 20), (32, 22),\n",
    "             (32, 23), (32, 29), (32, 30), (32, 31), (33, 8), (33, 9), (33, 13), (33, 14), (33, 15),\n",
    "             (33, 18), (33, 19), (33, 20), (33, 22), (33, 23), (33, 26), (33, 27), (33, 28),\n",
    "             (33, 29), (33, 30), (33, 31), (33, 32)]\n",
    "\n",
    "# >>> YOUR CODES START\n",
    "\n",
    "src, dst = tuple(zip(*edge_list))\n",
    "G.add_edges(src, dst)\n",
    "\n",
    "# <<< YOUR CODES END\n",
    "\n",
    "# We should have 78 edges now!\n",
    "print('Now we have %d edges!' % G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: manipulating node/edge features\n",
    "---------------------------------------------------------\n",
    "\n",
    "Nodes and edges in `DGLGraph` can have **features** tensors. Features of multiple nodes/edges are batched on the first dimension. Let's start by assigning a random feature vector of length 5 to all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.ndata['feat'] = torch.randn((34, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each node has a feature vector `'feat'` that has 5 elements. Note since there are 34 nodes in this graph, the first dimension must be of size 34, so that each row corresponds to the feature vector of each node. Error will be raised if the dimension mismatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will raise error!!\n",
    "# G.ndata['wrong_feat'] = torch.randn((35, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `G.ndata` is a dictionary-like structure, so it is compatible with any operation on dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `dict.update` to add new features (vector of length 3)\n",
    "G.ndata.update({'another_feat' : torch.randn((34, 3))})\n",
    "\n",
    "# Print the feature dictionary\n",
    "print(G.ndata)\n",
    "\n",
    "# Delete the new feature using `dict.pop`\n",
    "G.ndata.pop('another_feat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you might want to update features of some but not all of the nodes. This can be done using the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set node 0's feat to be all-zeros vector. Please be aware of the extra size 1 dimension here.\n",
    "G.nodes[0].data['feat'] = torch.zeros((1, 5))\n",
    "\n",
    "# Set node 2, 3's feat to be all-ones vector at once using list type.\n",
    "G.nodes[[2, 3]].data['feat'] = torch.ones((2, 5))\n",
    "\n",
    "# Set node 10, 11, 12's feat to be all-twos vector at once using tensor type.\n",
    "to_change = torch.tensor([10, 11, 12])\n",
    "G.nodes[to_change].data['feat'] = torch.ones((3, 5)) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `G.ndata` and `G.nodes`, we have `G.edata` and `G.edges` to access and modify edge features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The broness edge feature is just a scalar.\n",
    "G.edata['broness'] = torch.ones((G.number_of_edges(),))\n",
    "\n",
    "# The instructor (node 0) is a tough guy, so his friends are a little bit scared of him.\n",
    "G.edges[G.predecessors(0), 0].data['broness'] *= 0.5\n",
    "\n",
    "print(G.edata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: We know that measuring bro-ness cannot be accurate. Could you add some small random noise to it?\n",
    "# Hint: Use `torch.randn` to add small permutation to it.\n",
    "#\n",
    "# >>> YOUR CODES START\n",
    "\n",
    "G.edata['broness'] += torch.randn((G.number_of_edges(),)) * 0.1\n",
    "\n",
    "# <<< YOUR CODES END\n",
    "\n",
    "# You should see some randomness here\n",
    "print(G.edata['broness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: converting to/from networkx graph\n",
    "------------------------------------------------------------\n",
    "\n",
    "[Networkx](https://networkx.github.io/documentation/stable/) is a classical and popular python graph library. It provides many good utilities to analyze and visualize a graph. `DGLGraph` can be easily converted to/from `networkx` graph very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_G = G.to_networkx()\n",
    "pos = nx.circular_layout(nx_G)\n",
    "nx.draw(nx_G, pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing a DGLGraph from networkx is straight-forward. In fact, DGL borrows many of the networkx utilities to create graph from different format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from networkx graph\n",
    "G_from_nx = dgl.DGLGraph(nx_G)  # this gives you the same karate club network\n",
    "\n",
    "# from edge list\n",
    "G_from_elist = dgl.DGLGraph([(0,1), (1,2), (2,3)])  # this gives you a chain graph\n",
    "\n",
    "# from scipy sparse matrix\n",
    "import scipy.sparse as sp\n",
    "A = sp.eye(5, 5, 1)\n",
    "G_from_sp = dgl.DGLGraph(A)  # this also gives you a chain of 5 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Message passing on graph\n",
    "-------------------------------------------------\n",
    "\n",
    "Many graph neural networks follows the **message passing** computation model -- nodes can send out messages which are then aggregated and used to update the receiver nodes. We go through the basic mechanism of message passing using a toy task and then use it to implement a Graph Convolutional Network (GCN).\n",
    "\n",
    "Suppose the club president (node 33) is sending out an invitation of their annual karate match. The president also asks the club members to broadcast the news to, of course, their friends in the club. We use a scalar to represent whether the member has received the invitation or not (1 for invited, 0 for not invited). Initially, everyone is 0 except node 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first convert the uni-directional edges to bi-directional so messages can\n",
    "#   be sent in both direction.\n",
    "# We also add a self loop for each node for convenience.\n",
    "src, dst = G.edges()\n",
    "GG = dgl.DGLGraph()\n",
    "GG.add_nodes(34)\n",
    "GG.add_edges(src, dst)\n",
    "GG.add_edges(dst, src)\n",
    "# add self loop for each nodes\n",
    "v = G.nodes()\n",
    "GG.add_edges(v, v)\n",
    "print('We now have %d edges!' % GG.number_of_edges())\n",
    "\n",
    "# init the state\n",
    "GG.ndata['invited'] = torch.zeros((34,))\n",
    "GG.nodes[33].data['invited'] = torch.tensor([1.])\n",
    "print(GG.ndata['invited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the function that computes the messages. In DGL, the message function is an **Edge UDF** that takes in a single argument `edges`. It has three members `src`, `dst`, and `data` for accessing source node features, destination node features, and edge features respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_func(edges):\n",
    "    # The message is simply the 'invited' state of the source nodes.\n",
    "    return {'msg' : edges.src['invited']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the reduce function which accumulates and consume the messages to update the node features. In DGL, the reduce function is a **Node UDF** that takes in a single argument `nodes`, which has two members `data` and `mailbox`. `data` contains the node features while `mailbox` contains all incoming message features, stacked along the second dimension (hence the `dim=1` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_func(nodes):\n",
    "    # The reduce function sets the 'invited' state to be one if the node has already\n",
    "    #   been invited or any of the received messages contains an invitation (is one).\n",
    "    #   This can be done using sum and clamp operations as follows.\n",
    "    accum = nodes.mailbox['msg'].sum(dim=1)  # note that messages are stacked on dim=1\n",
    "    return {'invited' : accum.clamp(max=1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To trigger the message and reduce function, one can use the `send` and `recv` APIs. Following codes send out the messages from node 33:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first argument to `G.send` is the edges along which the messages are sent.\n",
    "# Note that we can use the same syntax used in adding edges to the graph.\n",
    "# The second argument is the message function we just defined.\n",
    "GG.send((33, GG.successors(33)), message_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call `recv` on the receiver nodes to trigger the reduce function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GG.recv(GG.successors(33), reduce_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print out the `'invited'` status to see the invitation being propagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(GG.ndata['invited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep doing so until all the nodes received the invitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_invited = int(torch.sum(GG.ndata['invited']))\n",
    "while num_invited != 34:\n",
    "    GG.send(GG.edges(), message_func)\n",
    "    GG.recv(GG.nodes(), reduce_func)\n",
    "    num_invited = int(torch.sum(GG.ndata['invited']))\n",
    "    print('%d members have been invited.' % num_invited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's under the hood?**\n",
    "\n",
    "The key idea here is to automatically batch the node and edge features so that your UDF can compute message passing on multiple nodes and edges in parallel.\n",
    "\n",
    "```python\n",
    "def message_func(edges):\n",
    "    return {'msg' : edges.src['invited']}\n",
    "```\n",
    "\n",
    "The `edges` argument is an `EdgeBatch` object representing a batch of edges. It has three members, `src`, `dst`, `data`. The `edges.src['invited']` returns a tensor of shape `(B,)`, where `B` is the number of edges being triggered.\n",
    "\n",
    "```python\n",
    "def reduce_func(nodes):\n",
    "    accum = nodes.mailbox['msg'].sum(dim=1)\n",
    "    return {'invited' : accum.clamp(max=1)}\n",
    "```\n",
    "\n",
    "Similarly, for the reduce function, the argument `nodes` is an `NodeBatch` object representing a batch of nodes. It has two members `data` and `mailbox`. The `nodes.mailbox['msg']` returns a tensor of shape `(B, deg)`, where `B` is the number of nodes that have the same in-degree `deg`. The reduce function will be called *many times* for each degree group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5: Implementing Graph Convolutional Network (GCN) in DGL\n",
    "--------------------------------------------------------------\n",
    "\n",
    "Graph convolutional network (GCN) is a popular model proposed by [Kipf & Welling](https://arxiv.org/abs/1609.02907) to encode graph structure by message passing. The high-level idea is similar to our toy task -- node features are updated by aggregating the messages from the neighbors. Here is its message passing equation:\n",
    "\n",
    "$$\n",
    "h_{v_i}^{(l+1)} = \\sigma \\left(\\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_{v_j}^{(l)}W^{(l)} \\right)\n",
    "$$\n",
    "\n",
    ", where $v_i$ is any node in the graph; $h_{v_i}$ is the feature of node $v_i$; $\\mathcal{N}(i)$ denotes the neighborhood of $v_i$; $c_{ij}$ is the normalization constant related to node degrees; $W$ is the parameter and $\\sigma$ is a non-linear activation function.\n",
    "\n",
    "The procedure to implement GCN in DGL is also similar to the toy task:\n",
    "* Define the message function.\n",
    "* Define the reduce function.\n",
    "* Define how they are triggered using `send` and `recv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the message & reduce function\n",
    "# NOTE: we ignore the normalization constant c_ij for now.\n",
    "def gcn_message(edges):\n",
    "    # messages are the features of the source nodes.\n",
    "    return {'msg' : edges.src['h']}\n",
    "\n",
    "def gcn_reduce(nodes):\n",
    "    # messages are summed\n",
    "    return {'h' : torch.sum(nodes.mailbox['msg'], dim=1)}\n",
    "\n",
    "# Define the GCN module\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "        # g is the graph and the inputs is the input node features\n",
    "        # first set the node features\n",
    "        g.ndata['h'] = inputs\n",
    "        # trigger message passing\n",
    "        g.send(g.edges(), gcn_message)\n",
    "        g.recv(g.nodes(), gcn_reduce)\n",
    "        # get the result node features\n",
    "        h = g.ndata.pop('h')\n",
    "        # perform linear transformation\n",
    "        return self.linear(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this model, let's try to predict which club member will join whose group (instructor or club president) after the split. We adopt the semi-supervised setting developed by Kipf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous features\n",
    "GG.ndata.clear()\n",
    "GG.edata.clear()\n",
    "\n",
    "# Define a 2-layer GCN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(in_feats, hidden_size)\n",
    "        self.gcn2 = GCN(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "        h = self.gcn1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = self.gcn2(g, h)\n",
    "        return h\n",
    "\n",
    "inputs = torch.eye(34)  # featureless inputs\n",
    "labeled_nodes = torch.tensor([0, 33])  # only the instructor and the president nodes are labeled\n",
    "labels = torch.tensor([0, 1])  # their labels are different\n",
    "net = Net(34, 5, 2)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "all_logits = []\n",
    "for epoch in range(30):\n",
    "    logits = net(GG, inputs)\n",
    "    all_logits.append(logits.detach())\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    # we only compute loss for node 0 and node 33\n",
    "    loss = F.nll_loss(logp[labeled_nodes], labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the node classification using the logits output.\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure(dpi=150)\n",
    "fig.clf()\n",
    "ax = fig.subplots()\n",
    "def draw(i):\n",
    "    cls1color = '#00FFFF'\n",
    "    cls2color = '#FF00FF'\n",
    "    pos = {}\n",
    "    colors = []\n",
    "    for v in range(34):\n",
    "        pos[v] = all_logits[i][v].numpy()\n",
    "        cls = np.argmax(pos[v])\n",
    "        colors.append(cls1color if cls else cls2color)\n",
    "    ax.cla()\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Epoch: %d' % i)\n",
    "    nx.draw(nx_G.to_undirected(), pos, node_color=colors, with_labels=True, node_size=500)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, draw, frames=len(all_logits), interval=200)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Topic: speed up GNN training\n",
    "--------------------------------------------------\n",
    "\n",
    "DGL provides many routines that combines basic `send` and `recv` in various ways. They are called **level-2 APIs**. For example, we can use the `update_all` API in the GCN module so that no explicit `edges()` and `nodes()` tensors are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the GCN module using level-2 APIs.\n",
    "class GCN_level2(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCN_level2, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "        # g is the graph and the inputs is the input node features\n",
    "        # first set the node features\n",
    "        g.ndata['h'] = inputs\n",
    "        # trigger message passing using `update_all`\n",
    "        # original codes:\n",
    "        #   g.send(g.edges(), gcn_message)\n",
    "        #   g.recv(g.nodes(), gcn_reduce)\n",
    "        g.update_all(gcn_message, gcn_reduce)\n",
    "        # get the result node features\n",
    "        h = g.ndata.pop('h')\n",
    "        # perform linear transformation\n",
    "        return self.linear(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some of the message and reduce functions are very commonly used, DGL also provides **builtin functions**. The following codes use `copy_src` and `sum` builtins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the GCN module using DGL builtin functions.\n",
    "import dgl.function as fn\n",
    "\n",
    "class GCN_builtin(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCN_builtin, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "    \n",
    "    def forward(self, g, inputs):\n",
    "        # g is the graph and the inputs is the input node features\n",
    "        # first set the node features\n",
    "        g.ndata['h'] = inputs\n",
    "        # trigger message passing using `update_all`\n",
    "        # original codes:\n",
    "        #   g.send(g.edges(), gcn_message)\n",
    "        #   g.recv(g.nodes(), gcn_reduce)\n",
    "        g.update_all(fn.copy_src('h', 'msg'), fn.sum('msg', 'h'))\n",
    "        # get the result node features\n",
    "        h = g.ndata.pop('h')\n",
    "        # perform linear transformation\n",
    "        return self.linear(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "There is still one missing piece. In our GCN model, \n",
    "$$\n",
    "h_{v_i}^{(l+1)} = \\sigma \\left(\\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_{v_j}^{(l)}W^{(l)} \\right)\n",
    "$$\n",
    "And we haven't implemented the normalizer $c_{ij}$. Kipf, in GCN paper, pointed out that the normalizer should be computed as follows:\n",
    "\n",
    "$$\n",
    "c_{ij} = \\sqrt{d_id_j}\n",
    "$$\n",
    "\n",
    ", where $d_i, d_j$ are the degrees of node $v_i$ and $v_j$ respectively. Your task is to modify the program to implement it.\n",
    "\n",
    "**Hint #1**: Use `GG.in_degrees(GG.nodes())` to get a 1-D tensor containing the degrees of all the nodes.\n",
    "\n",
    "**Hint #2**: Since $c_{ij}$ has a subscription $ij$, it is tied to the edges, and our message function is (not coincidently) an **edge UDF**.\n",
    "\n",
    "Have fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6: Batch execution of graphs\n",
    "------------------------------------------------------\n",
    "So far, we have learnt together how to create and change graph with DGL, and how to trigger computation on nodes and edges of a graph, which is useful to implement models that focus on representation learning on one single large graph like citation graph or knowledge graph.\n",
    "\n",
    "However, there are also scenarios like learning sentence syntax trees ([Tai et al., 2015](https://arxiv.org/abs/1503.00075)) or chemical structures ([Jin et at., 2018](https://arxiv.org/abs/1802.04364)) where the goal is to learn representation to classify or extract features for a lot of individual graphs. \n",
    "\n",
    "In such applications, ability to batch computation on multiple graphs matter. And we will demonstrate how DGL address batching graph with its BatchedGraph API.\n",
    "\n",
    "### Simple Graph Classification Task\n",
    "To make it more concrete, let's use graph classification as our example. Graph classification is an important problem with applications across many fields – bioinformatics, chemoinformatics, social network analysis, urban computing and cyber-security. Applying graph neural networks to this problem has been a popular approach recently ([Ying et al., 2018](https://arxiv.org/abs/1806.08804), [Cangea et al., 2018](https://arxiv.org/abs/1811.01287), [Knyazev et al., 2018](https://arxiv.org/abs/1811.09595), [Bianchi et al., 2019](https://arxiv.org/abs/1901.01343), [Liao et al., 2019](https://arxiv.org/abs/1901.01484), [Gao et al., 2019](https://openreview.net/forum?id=HJePRoAct7)).\n",
    "\n",
    "\n",
    "In this tutorial, we will use a simple graph classification task. We create a synthetic dataset data.MiniGCDataset, which has 8 different types of graphs and each class has the same number of graph samples. And the task is decide which of the 8 types below each graph belongs to.\n",
    "\n",
    "![](https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/dataset_overview.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import MiniGCDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "# A dataset with 80 samples, each graph is\n",
    "# of size [10, 20]\n",
    "dataset = MiniGCDataset(80, 10, 20)\n",
    "graph, label = dataset[0]\n",
    "fig, ax = plt.subplots()\n",
    "nx.draw(graph.to_networkx())\n",
    "ax.set_title('Class: {:d}'.format(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form a graph mini-batch\n",
    "To train neural networks more efficiently, we need to **batch** multiple samples together to form a mini-batch. Batching fixed-shaped tensor inputs is quite easy (for example, batching two images of size 28×28 gives a tensor of shape 2×28×28). By contrast, batching graph inputs has two challenges:\n",
    "\n",
    "- Graphs are sparse.\n",
    "- Graphs can have various length (e.g. number of nodes and edges).\n",
    "\n",
    "To address this, DGL provides a `dgl.batch()` API. It leverages the trick that a batch of graphs can be viewed as a large graph that have many disjoint connected components. Below is an illustration:\n",
    "\n",
    "![](https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/batch.png)\n",
    "\n",
    "We define the following `collate` function to form a mini-batch from a given list of graph and label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return type of `dgl.batch()` is still a graph (similar to the fact that a batch of tensors is still a tensor). This means that any code that works for one graph immediately works for a batch of graphs. More importantly, since DGL processes messages on all nodes and edges in parallel, this greatly improves efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Classifier\n",
    "The graph classification can be proceeded as follows:\n",
    "![](https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/graph_classifier.png)\n",
    "From a batch of graphs, we first perform message passing/graph convolution for nodes to “communicate” with others. After message passing, we compute a tensor for graph representation from node (and edge) attributes. This step may be called “readout/aggregation” interchangeably. Finally, the graph representations can be fed into a classifier g to predict the graph labels.\n",
    "\n",
    "### Graph Convolution\n",
    "\n",
    "Our graph convolution operation is basically the same as that for GCN (checkout our tutorial). The only difference is that we use a simpler normalization factor for aggregating messages: $c_{ij}=|N(v_i)|$. Therefore, the update equation becomes:\n",
    "$$h^{(l+1)}_{v_i}=ReLU(b^{(l)}+\\frac{1}{|N(v_i)|}\\sum\\limits_{v_j\\in N(v_i)}h^{(l)}_{v_j}W^{(l)})$$\n",
    "The replacement of summation by average is to balance nodes with different degrees, which gives a better performance for this experiment.\n",
    "\n",
    "Note that the self edges added in the dataset initialization allows us to include the original node feature $h^{(l)}_v$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Sends a message of node feature h.\n",
    "msg = fn.copy_src(src='h', out='m')\n",
    "\n",
    "def reduce(nodes):\n",
    "    \"\"\"Take an average over all neighbor node features hu and use it to\n",
    "    overwrite the original node feature.\"\"\"\n",
    "    accum = torch.mean(nodes.mailbox['m'], 1)\n",
    "    return {'h': accum}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Initialize the node features with h.\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout and Classification\n",
    "\n",
    "For this demonstration, we consider initial node features to be their degrees. After two rounds of graph convolution, we perform a graph readout by averaging over all node features for each graph in the batch\n",
    "\n",
    "$$h_g=\\frac{1}{|V|}\\sum\\limits_{v\\in V}h_v$$\n",
    "\n",
    "In DGL, `dgl.mean_nodes()` handles this task for a batch of graphs with variable size. We then feed our graph representations into a classifier with one linear layer to obtain pre-softmax logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCN(in_dim, hidden_dim, F.relu),\n",
    "            GCN(hidden_dim, hidden_dim, F.relu)])\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        # For undirected graphs, in_degree is the same as\n",
    "        # out_degree.\n",
    "        h = g.in_degrees().view(-1, 1).float()\n",
    "        for conv in self.layers:\n",
    "            h = conv(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Training\n",
    "We create a synthetic dataset of 400 graphs with 10 ~ 20 nodes. 320 graphs constitute a training set and 80 graphs constitute a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create training and test sets.\n",
    "trainset = MiniGCDataset(320, 10, 20)\n",
    "testset = MiniGCDataset(80, 10, 20)\n",
    "# Use PyTorch's DataLoader and the collate function\n",
    "# defined before.\n",
    "data_loader = DataLoader(trainset, batch_size=32, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "\n",
    "# Create model\n",
    "model = Classifier(1, 256, trainset.num_classes)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(80):\n",
    "    epoch_loss = 0\n",
    "    for iter, (bg, label) in enumerate(data_loader):\n",
    "        prediction = model(bg)\n",
    "        loss = loss_func(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve of a run is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('cross entropy averaged over minibatches')\n",
    "plt.plot(epoch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# Convert a list of tuples to two lists\n",
    "test_X, test_Y = map(list, zip(*testset))\n",
    "test_bg = dgl.batch(test_X)\n",
    "test_Y = torch.tensor(test_Y).float().view(-1, 1)\n",
    "probs_Y = torch.softmax(model(test_bg), 1)\n",
    "sampled_Y = torch.multinomial(probs_Y, 1)\n",
    "argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
    "    (test_Y == sampled_Y.float()).sum().item() / len(test_Y) * 100))\n",
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "    (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
